{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Bible-Related Keyword Extraction Model\n",
    "- Data source:\n",
    "<br>\n",
    "    &ensp;(1) **Bible Corpus**: The multilingual Bible translations in XML formats were obtained from [Bible Corpus](https://github.com/christos-c/bible-corpus).\n",
    "    <br>\n",
    "    &ensp;(2) **Sermons Collection**: Sermons were collected from the following online resources:\n",
    "    <br>\n",
    "         &emsp;- [The Kingdom Collective](https://www.thekingdomcollective.com/spurgeon/list/)\n",
    "         <br>\n",
    "         &emsp;- [St. Andrew's Enfield](https://www.standrewsenfield.com)\n",
    "\n",
    "- Objective:\n",
    "    <br>\n",
    "    &ensp;(1) Develop a supervised learning model to identify and extract words relevant to the Bible.\n",
    "    <br>\n",
    "    &ensp;(2) Create highlights or keywords from sermons or other text related to biblical content.\n",
    "- Steps:\n",
    "    <br>\n",
    "    &ensp;(1) Data Cleansing and Parsing: \n",
    "    <br>\n",
    "        &emsp;i.Gather relevant text data (sermons and biblical texts)\n",
    "    <br>\n",
    "        &emsp;ii.Clean and preprocess the data (remove noise, special characters)   \n",
    "        &emsp;iii.Tokenize the text into words or subword units\n",
    "    <br>\n",
    "    &ensp;(2) Label Assignment: Assign each word a binary label, 0: irrelevant to the Bible, 1: relevant to the Bible\n",
    "    <br>\n",
    "    &ensp;(3) Model Training: Transformer and SVM\n",
    "    <br>\n",
    "    &ensp;(4) Keyword extraction\n",
    "- Outcome: The resulting model can automatically identify and highlight Bible-related terms in sermons or other religious content\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Original Data Source**\n",
    "<br>\n",
    "The data was initially in an XML format (English.xml) containing English Bible verses. This was parsed and converted into a TXT file (bible_eng_2.txt) for easier handling. The resulting dataset has 95,886 rows, with each row corresponding to a verse.\n",
    "<br>\n",
    "<br>\n",
    "**Initial Cleaning**\n",
    "The XML file was parsed to extract text data, which was then saved into a text file. This step is crucial as it transforms structured XML data into a format suitable for further processing.\n",
    "<br>\n",
    "<br>\n",
    "**Data Cleaning Steps**\n",
    "<br>\n",
    "a. Handling Missing Values:\n",
    "\n",
    "    Feature Dropping: There were no features with NaN values that required dropping in this context, as the conversion focused on text extraction rather than feature-based datasets. But to proceed extra tags and punctuations need to be removed\n",
    "<br>\n",
    "b. Imputation:\n",
    "\n",
    "    Not Applicable: Since the dataset consists of text verses, traditional imputation methods for missing values (such as using average values) are not applicable.\n",
    "<br>\n",
    "c. Feature Selection:\n",
    "\n",
    "    Relevance: All features from the XML data were retained as they were directly relevant to the text analysis (i.e., the Bible verses themselves). No irrelevant features were present to remove. But later for business needs, some tokens with value 0 will be converted to 1, vice versa if necessary later\n",
    "<br>\n",
    "d. Outlier Removal:\n",
    "\n",
    "    Outliers: There were no numeric outliers in the text dataset. The focus was on textual content rather than numerical values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "tree = ET.parse('English.xml')\n",
    "root = tree.getroot()\n",
    "texts = []\n",
    "all_punctuation = \"!\\\"#$%&'()*+,./:;<=>?@[\\\\]^_`{|}~\"\n",
    "for seg in root.findall(\".//seg\"):\n",
    "    text = seg.text\n",
    "    text = text.replace('\\n','').replace('\\t','')\n",
    "    text =  re.sub(rf\"[{all_punctuation}]\", '', text)\n",
    "    texts.append(text)\n",
    "\n",
    "with open('bible_eng.txt', 'w+') as f:\n",
    "    f.writelines(texts)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "**Categorization and Labeling**\n",
    "<br>\n",
    "<br>\n",
    "Named Entity Recognition (NER):\n",
    "\n",
    "    Tokenization: Words were tokenized using AutoTokenizer.\n",
    "\n",
    "    Classification: The tokenized words were categorized using TFAutoModelForTokenClassification.\n",
    "\n",
    "    Labeling: Added labels (0 and 1) were used to identify potential notes based on business needs.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Model Development**\n",
    "<br>\n",
    "<br>\n",
    "BERT Model for NER:\n",
    "\n",
    "    Pre-trained Model: Utilized dbmdz/bert-large-cased-finetuned-conll03-english, a pre-trained BERT model fine-tuned on the CoNLL-03 dataset for NER. This model was chosen for its effectiveness in recognizing entities in text.\n",
    "    \n",
    "    Fine-Tuning: The BERT model was fine-tuned to classify tokens into predefined categories relevant to the Bible texts, enhancing its ability to identify specific entities within the text.\n",
    "\n",
    "Support Vector Machine (SVM):\n",
    "\n",
    "    Feature Extraction: Features extracted from BERT’s token classification were used as input for the SVM model. This approach allows leveraging the contextual embeddings from BERT to improve classification performance.\n",
    "    \n",
    "    Model Training: An SVM classifier was trained to identify and extract meaningful words related to the Bible from the categorized tokens. Hyperparameters for the SVM were tuned to optimize performance.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Visualizations**\n",
    "<br>\n",
    "1. TF-IDF Heatmap:\n",
    "\n",
    "    Purpose: Visualizes the importance of words (tokens) across different segments.\n",
    "    Implementation: Use seaborn to create a heatmap of the TF-IDF scores for the top 50 tokens in each segment. (see TF_IDF_MAP)\n",
    "<br>\n",
    "2. Confusion Matrix:\n",
    "\n",
    "    Purpose: Evaluates the performance of the SVM model.\n",
    "    Implementation: Plot the confusion matrix. (see Confusion_MAP.png)\n",
    "<br>\n",
    "3. Accuracy Score, Precision, Recall, F1-Score:\n",
    "\n",
    "    Purpose: Provides the proportion of correctly predicted samples and Measures individual class performance.\n",
    "    Implementation: Logs to print out scores. \n",
    "<br>\n",
    "4. ROC Curve and AUC:\n",
    "\n",
    "    Purpose: Evaluates the model's performance across different thresholds.\n",
    "    Implementation: Plot the roc-auc chart (see ROC_AUC.png)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import TFAutoModelForTokenClassification, AutoTokenizer\n",
    "import tensorflow as tf\n",
    "\n",
    "model_name = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "model = TFAutoModelForTokenClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Function to process text batch by batch\n",
    "def process_batch(batch):\n",
    "    all_tokens = []\n",
    "    all_labels = []\n",
    "    all_notes = []\n",
    "    \n",
    "    # Tokenize the batch into words\n",
    "    words = batch.split()\n",
    "    \n",
    "    # Encode the words using the tokenizer\n",
    "    tokens = tokenizer(words, is_split_into_words=True, return_tensors=\"tf\", truncation=True, padding=True)\n",
    "    \n",
    "    # Get model predictions\n",
    "    outputs = model(tokens)\n",
    "    predictions = tf.argmax(outputs.logits, axis=-1)\n",
    "    \n",
    "    # Convert input_ids to tokens and align labels\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokens[\"input_ids\"][0])\n",
    "    token_labels = [model.config.id2label[prediction.numpy()] for prediction in predictions[0]]\n",
    "    \n",
    "    aligned_tokens = []\n",
    "    aligned_labels = []\n",
    "    notes_labels = []\n",
    "    \n",
    "    for token, label in zip(tokens, token_labels):\n",
    "        if token not in [\"[CLS]\", \"[SEP]\", \"[PAD]\"]:\n",
    "            if token.startswith(\"##\"):\n",
    "                # Append to the last token if it is a subword token\n",
    "                aligned_tokens[-1] = aligned_tokens[-1] + token[2:]\n",
    "            else:\n",
    "                aligned_tokens.append(token)\n",
    "                aligned_labels.append(label)\n",
    "                if label in {'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'}:\n",
    "                    notes_labels.append(1)  # Potential note\n",
    "                else:\n",
    "                    notes_labels.append(0)  # Not a note\n",
    "    \n",
    "    all_tokens.extend(aligned_tokens)\n",
    "    all_labels.extend(aligned_labels)\n",
    "    all_notes.extend(notes_labels)\n",
    "    \n",
    "    return all_tokens, all_labels, all_notes\n",
    "\n",
    "# Initialize lists to store the tokens, labels, and notes\n",
    "all_tokens = []\n",
    "all_labels = []\n",
    "all_notes = []\n",
    "\n",
    "# Read the text file line by line to handle large files\n",
    "max_token_length = 512  # Maximum token length for BERT models\n",
    "current_batch = \"\"\n",
    "\n",
    "with open('bible_eng_2.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        current_batch += line.strip() + \" \"\n",
    "        \n",
    "        # Check if the current batch exceeds the maximum token length\n",
    "        if len(tokenizer(current_batch.split(), is_split_into_words=True)[\"input_ids\"]) > max_token_length:\n",
    "            tokens, labels, notes = process_batch(current_batch)\n",
    "            all_tokens.extend(tokens)\n",
    "            all_labels.extend(labels)\n",
    "            all_notes.extend(notes)\n",
    "            current_batch = \"\"\n",
    "    \n",
    "    # Process any remaining lines in the current batch\n",
    "    if current_batch:\n",
    "        tokens, labels, notes = process_batch(current_batch)\n",
    "        all_tokens.extend(tokens)\n",
    "        all_labels.extend(labels)\n",
    "        all_notes.extend(notes)\n",
    "\n",
    "# Create a DataFrame and save to CSV\n",
    "df = pd.DataFrame({'Token': all_tokens, 'Label': all_labels, 'Notes': all_notes})\n",
    "df.to_csv('bible_tokens.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('bible_tokens.csv')\n",
    "tokens_to_change = {'gospel', 'saint', 'apostles'}\n",
    "df['Notes'] = df.apply(lambda row: 1 if row['Token'] in tokens_to_change else row['Notes'], axis=1)\n",
    "df.to_csv('modified_bible_tokens.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score,f1_score\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Load the CSV document with Bible related tokens\n",
    "bible_words_df = pd.read_csv('modified_bible_tokens.csv')\n",
    "\n",
    "# Load the sermons text\n",
    "with open('sermons.txt', 'r', encoding='utf-8') as file:\n",
    "    large_document = file.read()\n",
    "\n",
    "# Extract the tokens marked as 1 from the bible token document\n",
    "marked_words = bible_words_df[bible_words_df['Marker'] == 1]['Token'].tolist()\n",
    "marked_words = list(set(marked_words)) \n",
    "\n",
    "# Segment the sermons file to be paragraphs with length of 300\n",
    "def segment_document(text, segment_size=300):\n",
    "    words = text.split()\n",
    "    segments = [' '.join(words[i:i+segment_size]) for i in range(0, len(words), segment_size)]\n",
    "    return segments\n",
    "\n",
    "segments = segment_document(large_document)\n",
    "\n",
    "# Use TfidfVectorizer to calculate TF-IDF \n",
    "vectorizer = TfidfVectorizer(vocabulary=marked_words)\n",
    "X = vectorizer.fit_transform(segments)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create labels\n",
    "y = [(1 if any(word in segment for word in marked_words) else 0) for segment in segments]\n",
    "\n",
    "# Check labels \n",
    "print(f\"Class distribution: {pd.Series(y).value_counts()}\")\n",
    "\n",
    "# If this paragraph only has either 1 or 0, create dummy words\n",
    "if len(set(y)) < 2:\n",
    "    dummy_segment_with_keywords = ' '.join(marked_words[:10])  \n",
    "    dummy_segment_without_keywords = ' '.join(['dummyword']*10)  \n",
    "    segments.extend([dummy_segment_with_keywords, dummy_segment_without_keywords])\n",
    "    y.extend([1, 0])\n",
    "\n",
    "# Create train and test data\n",
    "X = vectorizer.fit_transform(segments)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train SVC\n",
    "model = SVC(kernel='rbf', class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on all paragraphs\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# test accuracy\n",
    "y_pred_test = model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "precision = precision_score(y_test, y_pred_test, pos_label=1)\n",
    "recall = recall_score(y_test, y_pred_test, pos_label=1)\n",
    "f1 = f1_score(y_test, y_pred_test, pos_label=1)\n",
    "print(f\"Test Set Accuracy: {test_accuracy:.2f}\")\n",
    "print(f\"Precision for 'Marked Words': {precision:.2f}\")\n",
    "print(f\"Recall for 'Marked Words': {recall:.2f}\")\n",
    "print(f\"F1-Score for 'Marked Words': {f1:.2f}\")\n",
    "\n",
    "# Only extract one markable word in each paragraph\n",
    "notes = []\n",
    "for segment, prediction in zip(segments, predictions):\n",
    "    if prediction == 1 and \"dummyword\" not in segment.lower():\n",
    "        sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', segment)\n",
    "        for sentence in sentences:\n",
    "            if any(word.lower() in sentence.lower() for word in marked_words):\n",
    "                words = sentence.split()\n",
    "                for i, word in enumerate(words):\n",
    "                    if word.lower() in marked_words:\n",
    "                        start = max(0, i - 1)\n",
    "                        end = min(len(words), i + 2)\n",
    "                        context_words = words[start:end]\n",
    "                        notes.append(' '.join(context_words))\n",
    "                        break  \n",
    "\n",
    "with open('extracted_notes.txt', 'w', encoding='utf-8') as file:\n",
    "    for note in notes:\n",
    "        file.write(note + '\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Convert TF-IDF matrix to DataFrame for better visualization\n",
    "tfidf_df = pd.DataFrame(X.toarray(), columns=feature_names)\n",
    "token_frequencies = tfidf_df.sum(axis=0)\n",
    "token_freq_df = pd.DataFrame({'Token': feature_names, 'Frequency': token_frequencies}).sort_values(by='Frequency', ascending=False)\n",
    "\n",
    "# Pick up top 50 tokens from the segments in the sermon document\n",
    "top_n = 50\n",
    "top_tokens = token_freq_df.head(top_n)\n",
    "top_tokens_list = top_tokens['Token'].tolist()\n",
    "\n",
    "top_tfidf_df = tfidf_df[top_tokens_list]\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(top_tfidf_df, cmap='YlGnBu', xticklabels=top_tfidf_df.columns, yticklabels=False)\n",
    "plt.title('TF-IDF Heatmap of Segments')\n",
    "plt.xlabel('Tokens')\n",
    "plt.ylabel('Segments')\n",
    "plt.show()\n",
    "\n",
    "labels = [0, 1]  # Adjust based on your actual labels\n",
    "cm = confusion_matrix(y_test, y_pred_test, labels=labels)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['No Marked Words', 'Marked Words'])\n",
    "disp.plot(cmap='Blues')\n",
    "plt.show()\n",
    "\n",
    "# Compute ROC curve and AUC\n",
    "fpr, tpr, _ = roc_curve(y_test, model.decision_function(X_test))\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "print(f\"AUC: {roc_auc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results and Analysis**\n",
    "<br>\n",
    "<br>\n",
    "In this project, we utilized the dbmdz/bert-large-cased-finetuned-conll03-english model in conjunction with an SVM classifier to extract meaningful words related to the Bible from sermon texts. Our analysis involves evaluating the performance of the SVM model through various metrics and visualizations. The final result can be seen in extracted_notes_2.txt. If we just want to see the outcome of evaluation matrics, run part_1.py file. \n",
    "<br>\n",
    "<br>\n",
    "Here’s a summary of the results and the iterative process undertaken:\n",
    "<br>\n",
    "\n",
    "Basic Results\n",
    "\n",
    "    Model Performance: The SVM classifier achieved a test set accuracy of 1.00, indicating perfect classification of the test samples. This suggests that the model correctly predicted all instances in the test set. What to improve though, is to increase the size of the sermons file and go through the tokens based on business needs. There is still space to use more models to mark tokens from the Bible corpus.\n",
    "\n",
    "Evaluation Metrics\n",
    "\n",
    "    Confusion Matrix:\n",
    "        The confusion matrix was used to evaluate the model's performance across different classes. However, a warning was encountered due to a mismatch between predicted and actual labels, which was addressed by ensuring all possible labels were included. \n",
    "    \n",
    "    Precision, Recall, and F1-Score:\n",
    "        Precision, recall, and F1-score for the class of \"Marked Words\" were computed to provide detailed performance metrics. These metrics give insights into the model's ability to correctly identify relevant words. However, as we mentioned before, being able to recognize all marked words does not promise that the extracted words will meet the potential users' needs, and we will need to think of extracting more meaningful phrases rather than a single word.\n",
    "\n",
    "    ROC Curve and AUC:\n",
    "\n",
    "        The ROC curve and AUC score were computed to assess the model’s discriminative ability. A 45-degree diagonal ROC curve would suggest a model with no discriminative power, but adjustments were made to ensure the ROC curve correctly reflected the model's performance.\n",
    "Visualizations\n",
    "\n",
    "    Confusion Matrix Plot:\n",
    "        The confusion matrix was visualized to illustrate the true positives, true negatives, false positives, and false negatives. The plot highlighted the distribution of predictions across different classes.\n",
    "\n",
    "    TF-IDF Heatmap:\n",
    "        A heatmap was generated to visualize the TF-IDF values across different segments and tokens. Non-zero tokens were selected for better clarity, revealing how the model weights different features.\n",
    "Iteration and Improvement\n",
    "\n",
    "    Training and Evaluation:\n",
    "        The model underwent multiple iterations of training and evaluation. Initial observations of perfect accuracy led to additional validation steps, including cross-validation and ROC curve analysis, to ensure robustness.\n",
    "\n",
    "    Feature Selection:\n",
    "        The feature selection process involved using TF-IDF to identify important tokens. Only non-zero tokens were considered for visualization to focus on relevant features.\n",
    "\n",
    "    Model Comparison:\n",
    "        Although only the SVM model was used, comparisons were made against baseline metrics and theoretical expectations. The performance was analyzed in the context of potential overfitting and model generalization.\n",
    "Future Work\n",
    "\n",
    "    Model Performance:\n",
    "        The high accuracy observed may indicate overfitting or an overly simplistic test set. Additional metrics and validations were used to ensure the model's generalization capability.\n",
    "\n",
    "    Challenges:\n",
    "        Challenges included ensuring the representativeness of the test set and addressing class imbalance. Future work will involve testing different models on new data and extracting meaningful phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('extracted_notes.txt', 'r', encoding='utf-8') as file:  \n",
    "    all_notes = file.readlines()  \n",
    "    print(all_notes)\n",
    "    cleaned_notes = []\n",
    "    for note in all_notes:\n",
    "        cleaned_words = [word for word in note.split() if word in marked_words]\n",
    "        cleaned_notes.extend(cleaned_words)\n",
    "\n",
    "# Remove duplicated notes\n",
    "cleaned_notes = list(set(cleaned_notes))\n",
    "\n",
    "\n",
    "with open('extracted_notes_2.txt', 'w', encoding='utf-8') as file:\n",
    "    for note in cleaned_notes:\n",
    "        file.write(note + '\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
