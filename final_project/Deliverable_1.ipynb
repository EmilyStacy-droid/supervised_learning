{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Bible-Related Keyword Extraction Model\n",
    "- Data source: https://github.com/christos-c/bible-corpus (providing Bible in different languages)\n",
    "- Objective:\n",
    "    (1) Develop a supervised learning model to identify and extract words relevant to the Bible.\n",
    "    (2) Create highlights or keywords from sermons or other text related to biblical content.\n",
    "- Steps:\n",
    "    (1) Data Cleansing and Parsing: \n",
    "        i.Gather relevant text data (sermons and biblical texts)\n",
    "        ii.Clean and preprocess the data (remove noise, special characters)\n",
    "        iii.Tokenize the text into words or subword units\n",
    "    (2) Label Assignment: Assign each word a binary label, 0: irrelevant to the Bible, 1: relevant to the Bible\n",
    "    (3) Model Training: Transformer and SVM\n",
    "    (4) Keyword extraction\n",
    "- Outcome: The resulting model can automatically identify and highlight Bible-related terms in sermons or other religious content\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA Step 1: Parse the original xml file: the Bible in English and save the texts into a txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "tree = ET.parse('English.xml')\n",
    "root = tree.getroot()\n",
    "texts = []\n",
    "all_punctuation = \"!\\\"#$%&'()*+,./:;<=>?@[\\\\]^_`{|}~\"\n",
    "for seg in root.findall(\".//seg\"):\n",
    "    text = seg.text\n",
    "    text = text.replace('\\n','').replace('\\t','')\n",
    "    text =  re.sub(rf\"[{all_punctuation}]\", '', text)\n",
    "    texts.append(text)\n",
    "\n",
    "with open('bible_eng.txt', 'w+') as f:\n",
    "    f.writelines(texts)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA Step 2: Categorize the data with Named Entity Recognition\n",
    "1. tokenize each word with AutoTokenizer\n",
    "2. categorize word tokens with TFAutoModelForTokenClassification\n",
    "3. Add labels 0 and 1 (potential notes) with tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import TFAutoModelForTokenClassification, AutoTokenizer\n",
    "import tensorflow as tf\n",
    "\n",
    "model_name = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "model = TFAutoModelForTokenClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Function to process text batch by batch\n",
    "def process_batch(batch):\n",
    "    all_tokens = []\n",
    "    all_labels = []\n",
    "    all_notes = []\n",
    "    \n",
    "    # Tokenize the batch into words\n",
    "    words = batch.split()\n",
    "    \n",
    "    # Encode the words using the tokenizer\n",
    "    tokens = tokenizer(words, is_split_into_words=True, return_tensors=\"tf\", truncation=True, padding=True)\n",
    "    \n",
    "    # Get model predictions\n",
    "    outputs = model(tokens)\n",
    "    predictions = tf.argmax(outputs.logits, axis=-1)\n",
    "    \n",
    "    # Convert input_ids to tokens and align labels\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokens[\"input_ids\"][0])\n",
    "    token_labels = [model.config.id2label[prediction.numpy()] for prediction in predictions[0]]\n",
    "    \n",
    "    aligned_tokens = []\n",
    "    aligned_labels = []\n",
    "    notes_labels = []\n",
    "    \n",
    "    for token, label in zip(tokens, token_labels):\n",
    "        if token not in [\"[CLS]\", \"[SEP]\", \"[PAD]\"]:\n",
    "            if token.startswith(\"##\"):\n",
    "                # Append to the last token if it is a subword token\n",
    "                aligned_tokens[-1] = aligned_tokens[-1] + token[2:]\n",
    "            else:\n",
    "                aligned_tokens.append(token)\n",
    "                aligned_labels.append(label)\n",
    "                if label in {'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'}:\n",
    "                    notes_labels.append(1)  # Potential note\n",
    "                else:\n",
    "                    notes_labels.append(0)  # Not a note\n",
    "    \n",
    "    all_tokens.extend(aligned_tokens)\n",
    "    all_labels.extend(aligned_labels)\n",
    "    all_notes.extend(notes_labels)\n",
    "    \n",
    "    return all_tokens, all_labels, all_notes\n",
    "\n",
    "# Initialize lists to store the tokens, labels, and notes\n",
    "all_tokens = []\n",
    "all_labels = []\n",
    "all_notes = []\n",
    "\n",
    "# Read the text file line by line to handle large files\n",
    "max_token_length = 512  # Maximum token length for BERT models\n",
    "current_batch = \"\"\n",
    "\n",
    "with open('bible_eng_2.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        current_batch += line.strip() + \" \"\n",
    "        \n",
    "        # Check if the current batch exceeds the maximum token length\n",
    "        if len(tokenizer(current_batch.split(), is_split_into_words=True)[\"input_ids\"]) > max_token_length:\n",
    "            tokens, labels, notes = process_batch(current_batch)\n",
    "            all_tokens.extend(tokens)\n",
    "            all_labels.extend(labels)\n",
    "            all_notes.extend(notes)\n",
    "            current_batch = \"\"\n",
    "    \n",
    "    # Process any remaining lines in the current batch\n",
    "    if current_batch:\n",
    "        tokens, labels, notes = process_batch(current_batch)\n",
    "        all_tokens.extend(tokens)\n",
    "        all_labels.extend(labels)\n",
    "        all_notes.extend(notes)\n",
    "\n",
    "# Create a DataFrame and save to CSV\n",
    "df = pd.DataFrame({'Token': all_tokens, 'Label': all_labels, 'Notes': all_notes})\n",
    "df.to_csv('bible_tokens.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA step 3: Modify the labels value based on the buisness needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('bible_tokens.csv')\n",
    "tokens_to_change = {'gospel', 'saint'}\n",
    "df['Notes'] = df.apply(lambda row: 1 if row['Token'] in tokens_to_change else row['Notes'], axis=1)\n",
    "df.to_csv('modified_bible_tokens.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training: SVC\n",
    "With classifier 0 (not highlighted) and 1 (highlighted) from the bible text, use SVM to train a model that can extract meaningful words related to Bible from speeches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Load the CSV document with Bible related tokens\n",
    "bible_words_df = pd.read_csv('modified_bible_tokens.csv')\n",
    "\n",
    "# Load the sermons text\n",
    "with open('sermons.txt', 'r', encoding='utf-8') as file:\n",
    "    large_document = file.read()\n",
    "\n",
    "# Extract the tokens marked as 1 from the bible token document\n",
    "marked_words = bible_words_df[bible_words_df['Marker'] == 1]['Token'].tolist()\n",
    "marked_words = list(set(marked_words)) \n",
    "\n",
    "# Segment the sermons file to be paragraphs with length of 300\n",
    "def segment_document(text, segment_size=300):\n",
    "    words = text.split()\n",
    "    segments = [' '.join(words[i:i+segment_size]) for i in range(0, len(words), segment_size)]\n",
    "    return segments\n",
    "\n",
    "segments = segment_document(large_document)\n",
    "\n",
    "# Use TfidfVectorizer to calculate TF-IDF \n",
    "vectorizer = TfidfVectorizer(vocabulary=marked_words)\n",
    "X = vectorizer.fit_transform(segments)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create labels\n",
    "y = [(1 if any(word in segment for word in marked_words) else 0) for segment in segments]\n",
    "\n",
    "# Check labels \n",
    "print(f\"Class distribution: {pd.Series(y).value_counts()}\")\n",
    "\n",
    "# If this paragraph only has either 1 or 0, create dummy words\n",
    "if len(set(y)) < 2:\n",
    "    dummy_segment_with_keywords = ' '.join(marked_words[:10])  \n",
    "    dummy_segment_without_keywords = ' '.join(['dummyword']*10)  \n",
    "    segments.extend([dummy_segment_with_keywords, dummy_segment_without_keywords])\n",
    "    y.extend([1, 0])\n",
    "\n",
    "# Create train and test data\n",
    "X = vectorizer.fit_transform(segments)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train SVC\n",
    "model = SVC(kernel='sigmoid', degree=5)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on all paragraphs\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# Only extract one markable word in each paragraph\n",
    "notes = []\n",
    "for segment, prediction in zip(segments, predictions):\n",
    "    if prediction == 1 and \"dummyword\" not in segment.lower():\n",
    "        sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', segment)\n",
    "        for sentence in sentences:\n",
    "            if any(word.lower() in sentence.lower() for word in marked_words):\n",
    "                words = sentence.split()\n",
    "                for i, word in enumerate(words):\n",
    "                    if word.lower() in marked_words:\n",
    "                        start = max(0, i - 1)\n",
    "                        end = min(len(words), i + 2)\n",
    "                        context_words = words[start:end]\n",
    "                        notes.append(' '.join(context_words))\n",
    "                        break  \n",
    "\n",
    "with open('extracted_notes.txt', 'w', encoding='utf-8') as file:\n",
    "    for note in notes:\n",
    "        file.write(note + '\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up the notes. Remove those that are marked as 0 in bible_tokens and repetitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('extracted_notes.txt', 'r', encoding='utf-8') as file:  \n",
    "    all_notes = file.readlines()  \n",
    "    print(all_notes)\n",
    "    cleaned_notes = []\n",
    "    for note in all_notes:\n",
    "        cleaned_words = [word for word in note.split() if word in marked_words]\n",
    "        cleaned_notes.extend(cleaned_words)\n",
    "\n",
    "# Remove duplicated notes\n",
    "cleaned_notes = list(set(cleaned_notes))\n",
    "\n",
    "\n",
    "with open('extracted_notes_2.txt', 'w', encoding='utf-8') as file:\n",
    "    for note in cleaned_notes:\n",
    "        file.write(note + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sermons collections:\n",
    "https://www.thekingdomcollective.com/spurgeon/list/\n",
    "https://www.standrewsenfield.com\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
